{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv1BNRNB9V1e"
   },
   "source": [
    "Cargamos las imágenes en un array para tenerlas disponibles luego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos descargamos el dataset de OpenDataSoft\n",
    "!wget -O \"airbnb-listings.cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyK-jVlfKp71"
   },
   "source": [
    "Y a partir de aqui es vuestro trabajo!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos Explicados de lo que voy a hacer: (omito la carga y limpieza previa)\n",
    "    Carga de datos y exploración de los mismos:\n",
    "    Primero, debemos cargar los datos del dataset de Airbnb y explorarlos para entender la estructura de los datos, identificar valores perdidos, etc.\n",
    "\n",
    "    Preprocesamiento de los datos:\n",
    "    A continuación, realizamos el preprocesamiento de los datos, incluyendo la normalización de los datos numéricos y categóricos, la selección de las variables relevantes, la asignación de valores perdidos, y la separación de los datos en conjuntos de entrenamiento y prueba.\n",
    "\n",
    "    Modelo basado en imágenes:\n",
    "    Para construir el modelo basado en imágenes, primero necesitamos normalizar los valores de las imágenes, separar los datos en conjuntos de entrenamiento y prueba, y construir el modelo convolucional utilizando una de las opciones mencionadas anteriormente (fine-tuning, transfer learning o from scratch). Luego, compilamos y entrenamos el modelo optimizando los hiperparámetros y evitando el overfitting con técnicas como Dropout, max-pooling, regularización, etc.\n",
    "\n",
    "    Modelo basado en 1D:\n",
    "    Para construir el modelo basado en 1D, primero necesitamos seleccionar las variables numéricas relevantes, normalizar los datos, y construir un modelo de red neuronal 1D. Luego, compilamos y entrenamos el modelo optimizando los hiperparámetros y evitando el overfitting con técnicas como Dropout, regularización, etc.\n",
    "\n",
    "    Modelo híbrido:\n",
    "    Finalmente, podemos construir un modelo híbrido combinando los dos modelos previamente construidos. Podemos realizar el modelo híbrido utilizando alguna de las técnicas mencionadas anteriormente, como early fusion o late fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "#!wget -O \"airbnb-listings.csv\" \"https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\"\n",
    "airbnb_data = pd.read_csv('airbnb.csv')\n",
    "\n",
    "# Explorar los datos\n",
    "print(airbnb_data.head())\n",
    "print(airbnb_data.describe())\n",
    "\n",
    "#Con esto vemos los datos que tenemos columnas y demas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalización del valor a predecir:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "airbnb_data['precio_normalizado'] = scaler.fit_transform(airbnb_data[['precio']])\n",
    "\n",
    "#Selección de las variables que son de interés:\n",
    "\n",
    "features = ['imagen', 'barrio', 'num_habitaciones', 'num_baños', 'wifi', 'tv', 'aire_acondicionado', 'precio_normalizado']\n",
    "airbnb_data = airbnb_data[features]\n",
    "\n",
    "# Procesamiento de las variables numéricas y categóricas (normalización de datos):\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Normalización de las variables numéricas\n",
    "num_features = ['num_habitaciones', 'num_baños']\n",
    "num_scaler = StandardScaler()\n",
    "airbnb_data[num_features] = num_scaler.fit_transform(airbnb_data[num_features])\n",
    "\n",
    "# Codificación de las variables categóricas\n",
    "cat_features = ['barrio']\n",
    "cat_encoder = LabelEncoder()\n",
    "airbnb_data[cat_features] = cat_encoder.fit_transform(airbnb_data[cat_features])\n",
    "\n",
    "#Búsqueda y asignación de valores perdidos:\n",
    "\n",
    "airbnb_data.isna().sum()\n",
    "airbnb_data = airbnb_data.fillna(0)\n",
    "\n",
    "# Split de los datos\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = airbnb_data.drop(['precio_normalizado'], axis=1)\n",
    "y = airbnb_data['precio_normalizado']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Generación del modelo (optimización del número de densas, el número de neuronas):\n",
    "\n",
    "# Generación del modelo basado en imágenes\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "input_shape = (200, 200, 3)\n",
    "\n",
    "model_img = Sequential()\n",
    "model_img.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model_img.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_img.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_img.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_img.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model_img.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_img.add(Flatten())\n",
    "model_img.add(Dense(256, activation='relu'))\n",
    "model_img.add(Dropout(0.5))\n",
    "model_img.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Generación del modelo basado en datos 1D\n",
    "model_1d = Sequential()\n",
    "model_1d.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model_1d.add\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos procesado las variables numéricas y categóricas, es momento de generar el modelo y optimizar el número de capas densas y neuronas.\n",
    "\n",
    "Para construir el modelo, crearemos un modelo basado en 1D, donde las entradas serán los datos preprocesados y la salida será la variable objetivo (en este caso, el precio de la habitación).\n",
    "\n",
    "A continuación, intentare acabar dos capas densas y 64 neuronas cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera capa densa tiene 64 neuronas, función de activación ReLU y una entrada de tamaño X_train.shape[1]. \n",
    "\n",
    "La segunda capa densa también tiene 64 neuronas y función de activación ReLU. \n",
    "\n",
    "La última capa densa tiene una sola neurona, y la función de activación es lineal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos utilizado la función de pérdida mean_squared_error, que es adecuada para problemas de regresión, y el optimizador Adam con un learning rate de 0.001. También definimos el número de épocas y el tamaño de lote (batch size) para el entrenamiento.\n",
    "\n",
    "Finalmente, el modelo puede ser evaluado en el conjunto de datos de prueba y se puede calcular la precisión de la predicción. También se puede ajustar el modelo para mejorar su precisión y hacer predicciones más precisas.\n",
    "\n",
    "(Pendiente de acabar he dejado todo a medias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Para las imagenes\n",
    "\n",
    "#Entrenamiento del modelo de imágenes:\n",
    "image_model.fit(train_image_data, train_labels, epochs=50, batch_size=32, validation_data=(val_image_data, val_labels))\n",
    "\n",
    "#Entrenamiento del modelo 1D:\n",
    "numeric_model.fit(train_numeric_data, train_labels, epochs=50, batch_size=32, validation_data=(val_numeric_data, val_labels))\n",
    "\n",
    "# Fusión de modelos:\n",
    "\n",
    "combined_input = concatenate([image_model.output, numeric_model.output])\n",
    "x = Dense(64, activation='relu')(combined_input)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "combined_model = Model(inputs=[image_model.input, numeric_model.input], outputs=x)\n",
    "combined_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "combined_model.fit([train_image_data, train_numeric_data], train_labels, epochs=50, batch_size=32, validation_data=([val_image_data, val_numeric_data], val_labels))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
